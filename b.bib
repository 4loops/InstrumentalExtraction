%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Cole Peterson at 2016-01-25 10:26:46 -0800 


%% Saved with string encoding Unicode (UTF-8) 

@inproceedings{stoter20182018,
  title={The 2018 signal separation evaluation campaign},
  author={St{\"o}ter, Fabian-Robert and Liutkus, Antoine and Ito, Nobutaka},
  booktitle={International Conference on Latent Variable Analysis and Signal Separation},
  pages={293--305},
  year={2018},
  organization={Springer}
}

@inproceedings{DNNSpech2,
	Author = {Huang, Po-Sen and Kim, Minje and Hasegawa-Johnson, Mark and Smaragdis, Paris},
	Date-Added = {2016-01-25 17:00:17 +0000},
	Date-Modified = {2016-01-25 17:00:25 +0000},
	booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
	Pages = {1562--1566},
	Publisher = {IEEE},
	Title = {Deep learning for monaural speech separation},
	Ty = {CONF},
	Year = {2014}}


@inproceedings{DNNSpeech,
  title={Discriminatively trained recurrent neural networks for single-channel speech separation},
  author={Weninger, Felix and Hershey, John R and Le Roux, Jonathan and Schuller, Bjorn},
  booktitle={Signal and Information Processing (GlobalSIP), 2014 IEEE Global Conference on},
  pages={577--581},
  year={2014},
  organization={IEEE}
}

@Comment inproceedings{DNNSpeech,
 @Comment        Author = {Weninger, Felix and Hershey, John R and Le Roux, Jonathan and Schuller, Bjorn},
 @Comment        Date-Added = {2016-01-25 16:59:18 +0000},
 @Comment        Date-Modified = {2016-01-25 16:59:28 +0000},
 @Comment        Journal = {Signal and Information Processing (GlobalSIP), 2014 IEEE Global Conference on},
 @Comment        Pages = {577--581},
 @Comment        Publisher = {IEEE},
 @Comment        Title = {Discriminatively trained recurrent neural networks for single-channel speech separation},
 @Comment        Ty = {CONF},
 @Comment        Year = {2014}}

@phdthesis{DNNMulti,
  title={Multichannel audio source separation with deep neural networks},
  author={Nugraha, Aditya Arie and Liutkus, Antoine and Vincent, Emmanuel},
  year={2015},
  school={INRIA}
}


@Comment @article{DNNMulti,
@Comment 	Author = {Nugraha, Aditya Arie and Liutkus, Antoine and Vincent, Emmanuel},
@Comment 	Date-Added = {2016-01-25 16:58:11 +0000},
@Comment 	Date-Modified = {2016-01-25 16:58:25 +0000},
@Comment 	Publisher = {INRIA},
@Comment 	Title = {Multichannel audio source separation with deep neural networks},
@Comment 	Ty = {DISS},
@Comment 	Year = {2015}}

@inproceedings{DNNSing,
	Author = {Huang, Po-Sen and Kim, Minje and Hasegawa-Johnson, Mark and Smaragdis, Paris},
	Date-Added = {2016-01-25 16:55:12 +0000},
	Date-Modified = {2016-01-25 16:55:30 +0000},
	Title = {Singing-voice separation using deep recurrent neural networks},
        booktitle = {Music Information Retrieval Exchange (MIREX), 2014} 
	}


@inproceedings{DNNMusic,
  title={Deep neural network based instrument extraction from music},
  author={Uhlich, Stefan and Giron, Franck and Mitsufuji, Yuki},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},
  pages={2135--2139},
  year={2015},
  organization={IEEE}
}

@Comment @inproceedings{DNNMusic,
@Comment 	Author = {Uhlich, Stefan and Giron, Franck and Mitsufuji, Yuki},
@Comment 	Date-Added = {2016-01-25 16:53:22 +0000},
@Comment 	Date-Modified = {2016-01-25 16:54:00 +0000},
@Comment 	Journal = {Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},
@Comment 	Pages = {2135--2139},
@Comment 	Publisher = {IEEE},
@Comment 	Title = {Deep neural network based instrument extraction from music},
@Comment 	Ty = {CONF},
@Comment 	Year = {2015}}

@inbook{BSS,
	Abstract = {This article provides an overview of the first stereo audio source separation evaluation campaign, organized by the authors. Fifteen underdetermined stereo source separation algorithms have been applied to various audio data, including instantaneous, convolutive and real mixtures of speech or music sources. The data and the algorithms are presented and the estimated source signals are compared to reference signals using several objective performance criteria.},
	Address = {Berlin, Heidelberg},
	Author = {Vincent,Emmanuel and Sawada,Hiroshi and Bofill,Pau and Makino,Shoji and Rosca,Justinian P.},
	Date-Added = {2015-04-16 00:52:36 +0000},
	Date-Modified = {2015-04-16 00:52:45 +0000},
	Doi = {10.1007/978-3-540-74494-8{\_}69},
	Id = {1},
	Isbn = {9783540744931; 3540744932},
	Journal = {Independent Component Analysis and Signal Separation},
	Keywords = {Computation by Abstract Devices; Coding and Information Theory; Computer Science; Statistics and Computing/Statistics Programs; Algorithm Analysis and Problem Complexity; Signal, Image and Speech Processing; Data Mining and Knowledge Discovery},
	M1 = {Book, Section},
	Pages = {552--559},
	Publisher = {Springer Berlin Heidelberg},
	Title = {First Stereo Audio Source Separation Evaluation Campaign: Data, Algorithms and Results},
	Ty = {CHAP},
	U5 = {http://www.syndetics.com/index.aspx?isbn=9783540744931/sc.gif\&client=univvicss\&freeimage=true; http://www.syndetics.com/index.aspx?isbn=9783540744931/mc.gif\&client=univvicss\&freeimage=true; http://www.syndetics.com/index.aspx?isbn=9783540744931/lc.gif\&client=univvicss\&freeimage=true},
	U6 = {ctx{\_}ver=Z39.88-2004\&ctx{\_}enc=info{\%}3Aofi{\%}2Fenc{\%}3AUTF-8\&rfr{\_}id=info:sid/summon.serialssolutions.com\&rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:book\&rft.genre=book{\%}20item\&rft.title=Independent+Component+Analysis+and+Signal+Separation\&rft.au=Vincent{\%}2C+Emmanuel\&rft.au=Sawada{\%}2C+Hiroshi\&rft.au=Bofill{\%}2C+Pau\&rft.au=Makino{\%}2C+Shoji\&rft.atitle=First+Stereo+Audio+Source+Separation+Evaluation+Campaign{\%}3A+Data{\%}2C+Algorithms+and+Results\&rft.series=Lecture+Notes+in+Computer+Science\&rft.date=2007-01-01\&rft.pub=Springer+Berlin+Heidelberg\&rft.isbn=9783540744931\&rft.volume=4666\&rft.spage=552\&rft.epage=559\&rft{\_}id=info:doi/10.1007{\%}2F978-3-540-74494-8{\_}69\&rft.externalDBID=n{\%}2Fa\&rft.externalDocID=978-3-540-74494-8{\_}152140{\_}Chap69\&paramdict=en-US},
	U7 = {Book Chapter},
	Url = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwlV1dT4MwFL0xS0zUh-nU-JncHyBLgQKtb4sb2bMYX0kD3STOYgb8f2kZSMQXHklKc_vdntNzCuA6c2L9mRNSztyUJ0HdxonLKd_QlDmJCBhJEi6M2X5PTQZeh2Soz3lLUJp5u5W-_bL6AaWcWiz2tYrP9plxlFyuO6BF99p6nTroOnRq1znuIA_zbfc0df9lPGBJzeITTrsXVttLJx0T3Qhkhs6O4wp0Dmda8YBailDX8gUcSTWDafveAx6G_wxOe-aFl_AeZvXOEaM6icxxUaVZjpGhAjCSjaN4rnDVuYmjpjlEtlXPuBSleMLFbpvvs_Ljq0ChUnyVRbUriysIwtXby9pq44-_G0OMeBi73hVQEuvAfe5ew0TlSt4ACuFIoo90ZONToc3PAiYl8xxpp46XkFsgY3O_G__LPZw0EK1GUh5gUu4r-di4vv4ApYnBMQ},
	Volume = {4666},
	Year = {2007},
	Bdsk-Url-1 = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwlV1dT4MwFL0xS0zUh-nU-JncHyBLgQKtb4sb2bMYX0kD3STOYgb8f2kZSMQXHklKc_vdntNzCuA6c2L9mRNSztyUJ0HdxonLKd_QlDmJCBhJEi6M2X5PTQZeh2Soz3lLUJp5u5W-_bL6AaWcWiz2tYrP9plxlFyuO6BF99p6nTroOnRq1znuIA_zbfc0df9lPGBJzeITTrsXVttLJx0T3Qhkhs6O4wp0Dmda8YBailDX8gUcSTWDafveAx6G_wxOe-aFl_AeZvXOEaM6icxxUaVZjpGhAjCSjaN4rnDVuYmjpjlEtlXPuBSleMLFbpvvs_Ljq0ChUnyVRbUriysIwtXby9pq44-_G0OMeBi73hVQEuvAfe5ew0TlSt4ACuFIoo90ZONToc3PAiYl8xxpp46XkFsgY3O_G__LPZw0EK1GUh5gUu4r-di4vv4ApYnBMQ},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-3-540-74494-8%7B%5C_%7D69}}






@InBook{,
  ALTauthor =    {},
  ALTeditor =    {},
  title =        {},
  chapter =      {},
  publisher =    {},
  year =         {},
  OPTkey =       {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTtype =      {},
  OPTaddress =   {},
  OPTedition =   {},
  OPTmonth =     {},
  OPTpages =     {},
  OPTnote =      {},
  OPTannote =    {}
}





@article{Dannenberg:2008aa,
	Abstract = {Music is full of structure, including sections, sequences of distinct musical textures, and the repetition of phrases or entire sections. The analysis of music audio relies upon feature vectors that convey information about music texture or pitch content. Texture generally refers to the average spectral shape and statistical fluctuation, often reflecting the set of sounding instruments, e.g., strings, vocal, or drums. Pitch content reflects melody and harmony, which is often independent of texture. Structure is found in several ways. Segment boundaries can be detected by observing marked changes in locally averaged texture.},
	Address = {New York, NY},
	Author = {Dannenberg,Roger B. and Goto,Masataka},
	Date-Added = {2015-04-15 21:07:51 +0000},
	Date-Modified = {2015-04-15 21:07:51 +0000},
	Doi = {10.1007/978-0-387-30441-0{\_}21},
	Id = {1},
	Isbn = {0387776982; 9780387776989},
	journal = {Handbook of Signal Processing in Acoustics},
	Keywords = {Measurement Science, Instrumentation; Physics; Acoustics; Animal Physiology; Signal, Image and Speech Processing},
	M1 = {Book, Section},
	Pages = {305--331},
	Publisher = {Springer New York},
	title = {Music Structure Analysis from Acoustic Signals},
	Ty = {CHAP},
	U5 = {http://www.syndetics.com/index.aspx?isbn=9780387776989/sc.gif\&client=univvicss\&freeimage=true; http://www.syndetics.com/index.aspx?isbn=9780387776989/mc.gif\&client=univvicss\&freeimage=true; http://www.syndetics.com/index.aspx?isbn=9780387776989/lc.gif\&client=univvicss\&freeimage=true},
	U6 = {ctx{\_}ver=Z39.88-2004\&ctx{\_}enc=info{\%}3Aofi{\%}2Fenc{\%}3AUTF-8\&rfr{\_}id=info:sid/summon.serialssolutions.com\&rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:book\&rft.genre=book{\%}20item\&rft.title=Handbook+of+Signal+Processing+in+Acoustics\&rft.au=Dannenberg{\%}2C+Roger+B\&rft.au=Goto{\%}2C+Masataka\&rft.atitle=Music+Structure+Analysis+from+Acoustic+Signals\&rft.date=2008-01-01\&rft.pub=Springer+New+York\&rft.isbn=9780387776989\&rft.spage=305\&rft.epage=331\&rft{\_}id=info:doi/10.1007{\%}2F978-0-387-30441-0{\_}21\&rft.externalDBID=n{\%}2Fa\&rft.externalDocID=978-0-387-30441-0{\_}80414{\_}Chap21\&paramdict=en-US},
	U7 = {Book Chapter},
	Url = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwlV1LT8MwDLamSUiDAzBADIaUP7DSdHm0R4SYdoHLENeoeQwhpG4C8f9x0qYbdJfemkMS11LsxPb3GWCeJensn03ILPo5Jkp0T5ZK6kxh8YRSh8OCOZP-RZMBbyMZ1WcSE5TBbkfo244YFt_jDF_EKqDJqRC-ruv55a0NtOCdBm8ZssnUSimKPGuYd-K42MPUHVq4kyUNzmdx2nZYjUUnbSa6Bsh0mR37_dAZnHjEA_FQBNTyOQxcNYbjPabCMRyFSlHzfQFJ6AxNVoF39ufLkUhrQjxShTyYTegPRlYf756a-RLE4un1cTmL4qhtzW-huqJ4kiKmvBgZnV_BsNpU7hoIL4XRTGo81iXj3OVrzXVearQK0qbSTeC-5-IToIdmqFThDBVm4HejD9SQ2tr1Te9dbmFUl334SMoUhqgxd1ezvv4Cjc_ALA},
	Year = {2008},
	Bdsk-Url-1 = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwlV1LT8MwDLamSUiDAzBADIaUP7DSdHm0R4SYdoHLENeoeQwhpG4C8f9x0qYbdJfemkMS11LsxPb3GWCeJensn03ILPo5Jkp0T5ZK6kxh8YRSh8OCOZP-RZMBbyMZ1WcSE5TBbkfo244YFt_jDF_EKqDJqRC-ruv55a0NtOCdBm8ZssnUSimKPGuYd-K42MPUHVq4kyUNzmdx2nZYjUUnbSa6Bsh0mR37_dAZnHjEA_FQBNTyOQxcNYbjPabCMRyFSlHzfQFJ6AxNVoF39ufLkUhrQjxShTyYTegPRlYf756a-RLE4un1cTmL4qhtzW-huqJ4kiKmvBgZnV_BsNpU7hoIL4XRTGo81iXj3OVrzXVearQK0qbSTeC-5-IToIdmqFThDBVm4HejD9SQ2tr1Te9dbmFUl334SMoUhqgxd1ezvv4Cjc_ALA},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-0-387-30441-0%7B%5C_%7D21}}

@inproceedings{Foote:2000aa,
	Abstract = {The paper describes methods for automatically locating points of significant change in music or audio, by analyzing local self-similarity. This method can find individual note boundaries or even natural segment boundaries such as verse/chorus or speech/music transitions, even in the absence of cues such as silence. This approach uses the signal to model itself, and thus does not rely on particular acoustic cues nor requires training. We present a wide variety of applications, including indexing, segmenting, and beat tracking of music and audio. The method works well on a wide variety of audio sources},
	Author = {Foote,J.},
	Date-Added = {2015-04-15 21:05:25 +0000},
	Date-Modified = {2015-04-15 21:05:25 +0000},
	Doi = {10.1109/ICME.2000.869637},
	Id = {1},
	Isbn = {9780780365360; 0780365364},
	Journal = {2000 IEEE International Conference on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast Changing World of Multimedia (Cat. No.00TH8532)},
	Keywords = {music; audio signal processing; verse/chorus; indexing; beat tracking; multimedia systems; individual note boundaries; audio novelty measure; local self-similarity analysis; audio sources; signal sources; segmenting; speech/music transitions; fractals; acoustic cues; automatic audio segmentation; natural segment boundaries},
	M1 = {Conference Proceedings},
	Pages = {452--455 vol.1},
	Title = {Automatic audio segmentation using a measure of audio novelty},
	Ty = {CONF},
	U5 = {http://www.syndetics.com/index.aspx?isbn=9780780365360/sc.gif\&client=univvicss\&freeimage=true; http://www.syndetics.com/index.aspx?isbn=9780780365360/mc.gif\&client=univvicss\&freeimage=true; http://www.syndetics.com/index.aspx?isbn=9780780365360/lc.gif\&client=univvicss\&freeimage=true},
	U6 = {ctx{\_}ver=Z39.88-2004\&ctx{\_}enc=info{\%}3Aofi{\%}2Fenc{\%}3AUTF-8\&rfr{\_}id=info:sid/summon.serialssolutions.com\&rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:book\&rft.genre=proceeding\&rft.title=2000+IEEE+International+Conference+on+Multimedia+and+Expo.+ICME2000.+Proceedings.+Latest+Advances+in+the+Fast+Changing+World+of+Multimedia+{\%}28Cat.+No.00TH8532{\%}29\&rft.atitle=Automatic+audio+segmentation+using+a+measure+of+audio+novelty\&rft.au=Foote{\%}2C+J\&rft.date=2000-01-01\&rft.isbn=9780780365360\&rft.volume=1\&rft.spage=452\&rft.epage=455 vol.1\&rft{\_}id=info:doi/10.1109{\%}2FICME.2000.869637\&rft.externalDocID=6974\&paramdict=en-US},
	U7 = {Conference Proceeding},
	Url = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwpV1NT8IwGH5j8OIJRYz4kfQPAN3ase7AwSBEDiYevDcd64hxH4RtRv-9_RqI3uS2Jl3XLcvb9-t5HgDij_Dwl01IwoQSFTrHwluFykdIo4RFlEUr5Y9EzKi5_UCTQat6p_VOTCuaHOlLU9lX42os4oqLLDO0gmLrxDPYRP1NGlhOSKBVHCbLfboFB37ou8CdKasdkAl1_DvtGLdVTByNl7PnuUGxjOyiB9or5uhZdKHtdWtbTnZ1aAuP-cvr-J_XOYf-HvuHXnan2gWcyKIH3Vb8ATlb0INToxB9CdOHpi4N7ysSTfJWokqucwdoKpBuq18jgXKbikRl6mYV5YfM6q8-PC7mr7Onod4p31jqC25CBhxx_X20giXmdpfczPrMM25q2Xwj3rly5Si5gk5RFvIakPaN4tinMhAelSQRTPkuIvDCkKUpZnQAg4NH6WDLLL1J0gFMj9rGzZH338KZheDr1MsddOptI-8tTew34jvZ-g},
	Volume = {1},
	Year = {2000},
	Bdsk-Url-1 = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwpV1NT8IwGH5j8OIJRYz4kfQPAN3ase7AwSBEDiYevDcd64hxH4RtRv-9_RqI3uS2Jl3XLcvb9-t5HgDij_Dwl01IwoQSFTrHwluFykdIo4RFlEUr5Y9EzKi5_UCTQat6p_VOTCuaHOlLU9lX42os4oqLLDO0gmLrxDPYRP1NGlhOSKBVHCbLfboFB37ou8CdKasdkAl1_DvtGLdVTByNl7PnuUGxjOyiB9or5uhZdKHtdWtbTnZ1aAuP-cvr-J_XOYf-HvuHXnan2gWcyKIH3Vb8ATlb0INToxB9CdOHpi4N7ysSTfJWokqucwdoKpBuq18jgXKbikRl6mYV5YfM6q8-PC7mr7Onod4p31jqC25CBhxx_X20giXmdpfczPrMM25q2Xwj3rly5Si5gk5RFvIakPaN4tinMhAelSQRTPkuIvDCkKUpZnQAg4NH6WDLLL1J0gFMj9rGzZH338KZheDr1MsddOptI-8tTew34jvZ-g},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/ICME.2000.869637}}

@inproceedings{Foote:1999aa,
	Abstract = {This paper presents a novel approach to visualizing the time structure of music and audio. The acoustic similarity between any two instants of an audio recording is displayed in a 2D representation, allowing identification of structural and rhythmic characteristics. Examples are presented for classical and popular music. Applications include content-based analysis and segmentation, as well as tempo and structure extraction.},
	Author = {Foote,Jonathan},
	Date-Added = {2015-04-15 21:04:27 +0000},
	Date-Modified = {2015-04-15 21:04:27 +0000},
	Doi = {10.1145/319463.319472},
	Id = {1},
	Isbn = {1581131518; 9781581131512},
	Journal = {Proceedings of the seventh ACM international conference on multimedia (part 1)},
	Keywords = {audio analysis; audio similarity; music visualization},
	M1 = {Conference Proceedings},
	Pages = {77--80},
	Publisher = {ACM},
	Title = {Visualizing music and audio using self-similarity},
	Ty = {CONF},
	U5 = {http://www.syndetics.com/index.aspx?isbn=9781581131512/sc.gif\&client=univvicss\&freeimage=true; http://www.syndetics.com/index.aspx?isbn=9781581131512/mc.gif\&client=univvicss\&freeimage=true; http://www.syndetics.com/index.aspx?isbn=9781581131512/lc.gif\&client=univvicss\&freeimage=true},
	U6 = {ctx{\_}ver=Z39.88-2004\&ctx{\_}enc=info{\%}3Aofi{\%}2Fenc{\%}3AUTF-8\&rfr{\_}id=info:sid/summon.serialssolutions.com\&rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:book\&rft.genre=proceeding\&rft.title=Proceedings+of+the+seventh+ACM+international+conference+on+multimedia+{\%}28part+1{\%}29\&rft.atitle=Visualizing+music+and+audio+using+self-similarity\&rft.au=Foote{\%}2C+Jonathan\&rft.series=MULTIMEDIA+{\%}2799\&rft.date=1999-10-30\&rft.pub=ACM\&rft.isbn=9781581131512\&rft.spage=77\&rft.epage=80\&rft{\_}id=info:doi/10.1145{\%}2F319463.319472\&rft.externalDocID=319472\&paramdict=en-US},
	U7 = {Conference Proceeding},
	Url = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwjV3JTsMwEB2hfkApFFEWyT-QkNjO0iOqqLggcUBcK68oEk4RoRe-npkmbivKgaOjxLGseOZNZt4bAMHTLPllE2SFUEMab_XcuNxrzTUew6JS3ltbb0nwB2wyiJ3lBgiqTNgm9c0gW50aHyiBip-PrMj-YuBO5Vz3i6f9_5Vc1KWcE5erqHGAnq0eJJ7imEfNTVnc0VSlSPsZyVOZcOBnlmOI7JhYX7JLOvdcmGMRx3-t_RSme2Yfe975rAmcuPYMxrG1AxtO-jnkr01HdMtvvIsF6gXNVGuZ2thmzahW_o117t0nXRMajI0Ryk9htnx4WTwmuJTVR69hsepfLy5g1K5bdwkMkQpGq9Jm3HqJqEFnxtelEKry3LrCzGBy_PzVXxevcctJ24AMfHYDo6_PjbvtNVV_AI1zl-c},
	Year = {1999},
	Bdsk-Url-1 = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwjV3JTsMwEB2hfkApFFEWyT-QkNjO0iOqqLggcUBcK68oEk4RoRe-npkmbivKgaOjxLGseOZNZt4bAMHTLPllE2SFUEMab_XcuNxrzTUew6JS3ltbb0nwB2wyiJ3lBgiqTNgm9c0gW50aHyiBip-PrMj-YuBO5Vz3i6f9_5Vc1KWcE5erqHGAnq0eJJ7imEfNTVnc0VSlSPsZyVOZcOBnlmOI7JhYX7JLOvdcmGMRx3-t_RSme2Yfe975rAmcuPYMxrG1AxtO-jnkr01HdMtvvIsF6gXNVGuZ2thmzahW_o117t0nXRMajI0Ryk9htnx4WTwmuJTVR69hsepfLy5g1K5bdwkMkQpGq9Jm3HqJqEFnxtelEKry3LrCzGBy_PzVXxevcctJ24AMfHYDo6_PjbvtNVV_AI1zl-c},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/319463.319472}}

@article{Vincent:2012ab,
	Author = {Vincent,Emmanuel and Araki,Shoko and Theis,Fabian and Nolte,Guido and Bofill,Pau and Sawada,Hiroshi and Ozerov,Alexey and Gowreesunker,Vikrham and Lutter,Dominik and Duong,Ngoc Q. K.},
	Date-Added = {2015-04-15 21:00:24 +0000},
	Date-Modified = {2015-04-15 21:00:24 +0000},
	Doi = {10.1016/j.sigpro.2011.10.007},
	Id = {1},
	Isbn = {0165-1684},
	Journal = {Signal Processing},
	Journal1 = {Signal Process},
	Keywords = {Evaluation; Biomedical; Audio; Resources; Source separation},
	M1 = {Journal Article},
	Number = {8},
	Pages = {1928--1936},
	Publisher = {Elsevier B.V},
	Title = {The signal separation evaluation campaign (2007--2010): Achievements and remaining challenges},
	Ty = {JOUR},
	U5 = {http://www.syndetics.com/index.aspx?isbn=/sc.gif\&issn=0165-1684\&client=univvicss; http://www.syndetics.com/index.aspx?isbn=/mc.gif\&issn=0165-1684\&client=univvicss; http://www.syndetics.com/index.aspx?isbn=/lc.gif\&issn=0165-1684\&client=univvicss},
	U6 = {ctx{\_}ver=Z39.88-2004\&ctx{\_}enc=info{\%}3Aofi{\%}2Fenc{\%}3AUTF-8\&rfr{\_}id=info:sid/summon.serialssolutions.com\&rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:journal\&rft.genre=article\&rft.atitle=The+signal+separation+evaluation+campaign+{\%}282007-2010{\%}29{\%}3A+Achievements+and+remaining+challenges\&rft.jtitle=Signal+Processing\&rft.date=2012-08-01\&rft.pub=Elsevier+B.V\&rft.issn=0165-1684\&rft.eissn=1872-7557\&rft.volume=92\&rft.issue=8\&rft.spage=1928\&rft.externalDBID=n{\%}2Fa\&rft.externalDocID=282642785\&paramdict=en-US},
	U7 = {Journal Article},
	Url = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwpV09T8MwELVQJxj4rigfkkcYQosTxwlbhaiQEGIAVqKzc4YgkVZNu_Mf-If8Es5xUihsMMa6RLLPeueL371jLBSng-AHJqQojABJh1nII0WeQQuxzoHiKyipcbmazJfGNCTLJhJ4hK-xuxnpN2vbnxRF_86V5ZzFSeQk0MK4VgilSOX2_PXtzQKbaWTg1b5l4KzbYrqa8VUVT4RaXtazZnyppQjVaelyTfAZbbCWeNCSThY30b5A5rey4z8mtMnWmzMqH3q7LbaC5TZb-6ZcuMMeaXtxx_0guwq9fvi45F_a4dw4JUcy4MfuP-XH27u7ED8550PzXGAtUj6rOJQ5n-Kr71LBTdvYpdplD6PL-4uroGnVECAlfHGQD0Ci0UIDHUFsKjAEBZR6KxuJSKWEJCFGkc4tOL06jbmNtTBW0qNJckKdLuuU4xL3GAebGLASlbHCvaJjnYJJbRJRaig19JhqfZItLWVGQSBrSWsvmfdm5rzpRmmqPdZ1LswmXscjo2wzdp1G5P6fP3nAVulJeEbgIevMpnM88vqxn2Zq5lw},
	Volume = {92},
	Year = {2012},
	Bdsk-Url-1 = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwpV09T8MwELVQJxj4rigfkkcYQosTxwlbhaiQEGIAVqKzc4YgkVZNu_Mf-If8Es5xUihsMMa6RLLPeueL371jLBSng-AHJqQojABJh1nII0WeQQuxzoHiKyipcbmazJfGNCTLJhJ4hK-xuxnpN2vbnxRF_86V5ZzFSeQk0MK4VgilSOX2_PXtzQKbaWTg1b5l4KzbYrqa8VUVT4RaXtazZnyppQjVaelyTfAZbbCWeNCSThY30b5A5rey4z8mtMnWmzMqH3q7LbaC5TZb-6ZcuMMeaXtxx_0guwq9fvi45F_a4dw4JUcy4MfuP-XH27u7ED8550PzXGAtUj6rOJQ5n-Kr71LBTdvYpdplD6PL-4uroGnVECAlfHGQD0Ci0UIDHUFsKjAEBZR6KxuJSKWEJCFGkc4tOL06jbmNtTBW0qNJckKdLuuU4xL3GAebGLASlbHCvaJjnYJJbRJRaig19JhqfZItLWVGQSBrSWsvmfdm5rzpRmmqPdZ1LswmXscjo2wzdp1G5P6fP3nAVulJeEbgIevMpnM88vqxn2Zq5lw},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.sigpro.2011.10.007}}





@inproceedings{Haddad:2008aa,
	Abstract = {Blind source separation methods resort to very weak hypothesis concerning the source signals, as well as the mixing matrix. This paper verifies experimentally the performance improvement in two different source separation algorithmswhen some statistical knowledge about the mixing matrix is used. A natural way of inserting such information in source separation methods is to put them in a Bayesian framework. This approach presents immediate applications in digital communication and speech signal processing systems, among many others.},
	Author = {Haddad,D. B. and Haddad,Diego Barreto and Petraglia,M. R. and Petraglia,Mariane Rembold and Batalheiro,P. B. and Batalheiro,Paulo Bulkool},
	Date-Added = {2015-04-15 20:23:46 +0000},
	Date-Modified = {2015-04-15 20:23:46 +0000},
	Doi = {10.1109/SPAWC.2008.4641604},
	Id = {1},
	Isbn = {1424420458; 9781424420452},
	Booktitle = {2008 IEEE 9th Workshop on Signal Processing Advances in Wireless Communications},
	Keywords = {Bayesian methods; Signal processing algorithms; Equations; Monte Carlo methods; Mathematical model; semiblind source separation method; Blind source separation; matrix algebra; Bayesian framework; Bayes methods; Source separation; speech signal processing; digital communication; speech processing},
	M1 = {Conference Proceedings},
	Pages = {231--235},
	Publisher = {IEEE},
	Title = {Performance evaluation of two semi-blind source separation methods},
	Ty = {CONF},
	U5 = {http://www.syndetics.com/index.aspx?isbn=9781424420452/sc.gif\&client=univvicss\&freeimage=true; http://www.syndetics.com/index.aspx?isbn=9781424420452/mc.gif\&client=univvicss\&freeimage=true; http://www.syndetics.com/index.aspx?isbn=9781424420452/lc.gif\&client=univvicss\&freeimage=true},
	U6 = {ctx{\_}ver=Z39.88-2004\&ctx{\_}enc=info{\%}3Aofi{\%}2Fenc{\%}3AUTF-8\&rfr{\_}id=info:sid/summon.serialssolutions.com\&rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:book\&rft.genre=proceeding\&rft.title=2008+IEEE+9th+Workshop+on+Signal+Processing+Advances+in+Wireless+Communications\&rft.atitle=Performance+evaluation+of+two+semi-blind+source+separation+methods\&rft.au=Haddad{\%}2C+D.B\&rft.au=Haddad{\%}2C+Diego+Barreto\&rft.au=Petraglia{\%}2C+M.R\&rft.au=Petraglia{\%}2C+Mariane+Rembold\&rft.date=2008-01-01\&rft.pub=IEEE\&rft.isbn=9781424420452\&rft.spage=231\&rft.epage=235\&rft{\_}id=info:doi/10.1109{\%}2FSPAWC.2008.4641604\&rft.externalDocID=4634580\&paramdict=en-US},
	U7 = {Conference Proceeding},
	Url = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwnV1dS8MwFA2yJ59UnFh10D-wLW3SNHkSHYq-DRz4GPJxA-K-WCf6802TdqX6IPhQaJqQkgaak9x7zkGI5BM8_vFPsADOOZNZ5RGs1VyVYPzCrkxhcsdK1WeTodZlsfY7CaloMKlvQ2Tfl6up0pVUy2WQFVS7xjyDMo8ugjYoIUVt48Ceu_MWD10II7jldtUq7LyVfGrKeUuqwWL6Mr97ncVUy6bbnv1KWH0eT1CbY9tmnRxC0ZEh81va8V8jOkXDjv-Xzg8r2xk6gvU5up93HIO0EwlPNy7df27SClZ-m-1Bq01jQMA_ibrivk30qa6GaPH4sJg9jRsHhvEbrynmJjeOMOEvTnjJocwKAxw040p46EMxoZYWWmfMeiShmbCYgWBCZ5pb7MgFGqw3a7hEKYiyyEsgBqiiFpeCOT8foJzzkMXvWRKU1N9DbqPGhsTN0OXWugSN-nUB2TLKsdGCG52g21592NdgIcMcRp_NtrfQ7mu1lIEULLfqXeaCFVd_vOEaHcc8kfro5QYN9rsPGEWZ2G8oBdkQ},
	Year = {2008},
	Bdsk-Url-1 = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwnV1dS8MwFA2yJ59UnFh10D-wLW3SNHkSHYq-DRz4GPJxA-K-WCf6802TdqX6IPhQaJqQkgaak9x7zkGI5BM8_vFPsADOOZNZ5RGs1VyVYPzCrkxhcsdK1WeTodZlsfY7CaloMKlvQ2Tfl6up0pVUy2WQFVS7xjyDMo8ugjYoIUVt48Ceu_MWD10II7jldtUq7LyVfGrKeUuqwWL6Mr97ncVUy6bbnv1KWH0eT1CbY9tmnRxC0ZEh81va8V8jOkXDjv-Xzg8r2xk6gvU5up93HIO0EwlPNy7df27SClZ-m-1Bq01jQMA_ibrivk30qa6GaPH4sJg9jRsHhvEbrynmJjeOMOEvTnjJocwKAxw040p46EMxoZYWWmfMeiShmbCYgWBCZ5pb7MgFGqw3a7hEKYiyyEsgBqiiFpeCOT8foJzzkMXvWRKU1N9DbqPGhsTN0OXWugSN-nUB2TLKsdGCG52g21592NdgIcMcRp_NtrfQ7mu1lIEULLfqXeaCFVd_vOEaHcc8kfro5QYN9rsPGEWZ2G8oBdkQ},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/SPAWC.2008.4641604}}

@article{Vincent:2006aa,
	Abstract = {In this paper, we discuss the evaluation of blind audio source separation (BASS) algorithms. Depending on the exact application, different distortions can be allowed between an estimated source and the wanted true source. We consider four different sets of such allowed distortions, from time-invariant gains to time-varying filters. In each case, we decompose the estimated source into a true source part plus error terms corresponding to interferences, additive noise, and algorithmic artifacts. Then, we derive a global performance measure using an energy ratio, plus a separate performance measure for each error term. These measures are computed and discussed on the results of several BASS problems with various difficulty levels},
	Address = {New York},
	Author = {Vincent,E. and Gribonval,R. and Fevotte,C.},
	Date-Added = {2015-04-15 20:17:18 +0000},
	Date-Modified = {2015-04-15 20:17:18 +0000},
	Doi = {10.1109/TSA.2005.858005},
	Id = {1},
	Isbn = {1558-7916},
	Journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	Keywords = {Time-varying filters; audio signal processing; measure; blind source separation; evaluation; distortion; blind audio source separation; performance; quality; algorithmic artifacts; distortions; time-invariant gains; interference; additive noise; source estimation; Audio source separation},
	M1 = {Journal Article},
	Number = {4},
	Pages = {1462--1469},
	Publisher = {IEE},
	Title = {Performance measurement in blind audio source separation},
	Ty = {JOUR},
	U5 = {http://www.syndetics.com/index.aspx?isbn=/sc.gif\&issn=1558-7916\&client=univvicss; http://www.syndetics.com/index.aspx?isbn=/mc.gif\&issn=1558-7916\&client=univvicss; http://www.syndetics.com/index.aspx?isbn=/lc.gif\&issn=1558-7916\&client=univvicss},
	U6 = {ctx{\_}ver=Z39.88-2004\&ctx{\_}enc=info{\%}3Aofi{\%}2Fenc{\%}3AUTF-8\&rfr{\_}id=info:sid/summon.serialssolutions.com\&rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:journal\&rft.genre=article\&rft.atitle=Performance+measurement+in+blind+audio+source+separation\&rft.jtitle=IEEE+Transactions+on+Audio{\%}2C+Speech{\%}2C+and+Language+Processing\&rft.au=Vincent{\%}2C+E\&rft.au=Vincent{\%}2C+E\&rft.au=Gribonval{\%}2C+R\&rft.au=Fevotte{\%}2C+C\&rft.date=2006\&rft.pub=IEE\&rft.issn=1558-7916\&rft.eissn=1558-7924\&rft.volume=14\&rft.issue=4\&rft.spage=1462\&rft.epage=1469\&rft{\_}id=info:doi/10.1109{\%}2FTSA.2005.858005\&rft.externalDocID=10376\&paramdict=en-US},
	U7 = {Journal Article},
	Url = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwnV1LSwMxEB6kJz34fqxa2KOXttnNoxvwUsTiUbCeQ7ZJQNxuSx_gz3eSdFuriOBtlzxgk-zkS2bm-wBo3iWdbzZBO081ZX2Qbp8Q531DgtKca7SGWe74bjYZNCqLXu8khKLZrn8Mnn18X_R0uVC6qgKtoJ6vxTMQ-VMR8smznPCYy9VYZSxhkTuVF56iUaxpfjIie6OXQbxcKTjCJ76zQwXJlR92Omw-wyNoQmyboJONJzomyPxkdvzXBx3D4RqdpoO4nE5gz9ancPCFs_AMiudtqkE62d4wpm91WiJmNalembdpGr0C6cJGcvFpfQ6vw8fRw1NnLb_QGXtW-U4xNi7v42YmuTVUUsacyAxxCLpKk4tCWEmJtTijUmqb941jxpSOMSNNJp2gF9Cqp7W9glRoPJeQUlLLNLNcIOwTzssVaSp9-msCd82Qq1lk2VDhdEKkwtnxWplcxdlJoO1HblONBJgrWIHWhDGGXd3vlP_SjQqVPiZVCOoTaqbfFcNl84_mOaWhuaf2S-C2WSkKp3TpM90WSvpjn9cDSuByU2yqSiHe9GLP-A9c__FZN7C_vfy5hdZyvrLtSFT7CXSx_y4},
	Volume = {14},
	Year = {2006},
	Bdsk-Url-1 = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwnV1LSwMxEB6kJz34fqxa2KOXttnNoxvwUsTiUbCeQ7ZJQNxuSx_gz3eSdFuriOBtlzxgk-zkS2bm-wBo3iWdbzZBO081ZX2Qbp8Q531DgtKca7SGWe74bjYZNCqLXu8khKLZrn8Mnn18X_R0uVC6qgKtoJ6vxTMQ-VMR8smznPCYy9VYZSxhkTuVF56iUaxpfjIie6OXQbxcKTjCJ76zQwXJlR92Omw-wyNoQmyboJONJzomyPxkdvzXBx3D4RqdpoO4nE5gz9ancPCFs_AMiudtqkE62d4wpm91WiJmNalembdpGr0C6cJGcvFpfQ6vw8fRw1NnLb_QGXtW-U4xNi7v42YmuTVUUsacyAxxCLpKk4tCWEmJtTijUmqb941jxpSOMSNNJp2gF9Cqp7W9glRoPJeQUlLLNLNcIOwTzssVaSp9-msCd82Qq1lk2VDhdEKkwtnxWplcxdlJoO1HblONBJgrWIHWhDGGXd3vlP_SjQqVPiZVCOoTaqbfFcNl84_mOaWhuaf2S-C2WSkKp3TpM90WSvpjn9cDSuByU2yqSiHe9GLP-A9c__FZN7C_vfy5hdZyvrLtSFT7CXSx_y4},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/TSA.2005.858005}}


@inproceedings{fasst2,
  title={Advances in audio source seperation and multisource audio content retrieval},
  author={Vincent, Emmanuel},
  booktitle={SPIE Defense, Security, and Sensing},
  pages={840109--840109},
  year={2012},
  organization={International Society for Optics and Photonics}
}


@Comment @inproceedings{fasst2,
@Comment 	Annote = {10.1117/12.926411},
@Comment 	Author = {Vincent, Emmanuel},
@Comment 	Date-Added = {2015-04-15 20:12:42 +0000},
@Comment 	Date-Modified = {2015-04-15 20:17:31 +0000},
@Comment 	M3 = {doi: 10.1117/12.926411},
@Comment 	N2 = {Audio source separation aims to extract the signals of individual sound sources from a given recording. In this paper, we review three recent advances which improve the robustness of source separation in real-world challenging scenarios and enable its use for multisource content retrieval tasks, such as automatic speech recognition (ASR) or acoustic event detection (AED) in noisy environments. We present a Flexible Audio Source Separation Toolkit (FASST) and discuss its advantages compared to earlier approaches such as independent component analysis (ICA) and sparse component analysis (SCA). We explain how cues as diverse as harmonicity, spectral envelope, temporal fine structure or spatial location can be jointly exploited by this toolkit. We subsequently present the uncertainty decoding (UD) framework for the integration of audio source separation and audio content retrieval. We show how the uncertainty about the separated source signals can be accurately estimated and propagated to the features. Finally, we explain how this uncertainty can be efficiently exploited by a classifier, both at the training and the decoding stage. We illustrate the resulting performance improvements in terms of speech separation quality and speaker recognition accuracy.},
@Comment 	Pages = {840109--840109-6},
@Comment 	Title = {Advances in audio source seperation and multisource audio content retrieval},
@Comment 	Ty = {CONF},
@Comment 	Url = {http://dx.doi.org/10.1117/12.926411},
@Comment 	Volume = {8401},
@Comment 	Year = {2012},
@Comment 	Bdsk-Url-1 = {http://dx.doi.org/10.1117/12.926411}}

@inbook{bayes,
	Abstract = {In this chapter we describe a Bayesian approach to audio source separation. The approach relies on probabilistic modeling of sound sources as (sparse) linear combinations of atoms from a dictionary and Markov chain Monte Carlo (MCMC) inference. Several prior distributions are considered for the source expansion coefficients. We first consider independent and identically distributed (iid) general priors with two choices of distributions. The first one is the Student t, which is a good model for sparsity when the shape parameter has a low value. The second one is a hierarchical mixture distribution; conditionally upon an indicator variable, one coefficient is either set to zero or given a normal distribution, whose variance is in turn given an inverted-Gamma distribution. Then, we consider more audiospecific models where both the identically distributed and independently distributed assumptions are lifted. Using a Modified Discrete Cosine Transform (MDCT) dictionary, a time--frequency orthonormal basis, we describe frequency-dependent structured priors which explicitly model the harmonic structure of sound, using a Markov hierarchical modeling of the expansion coefficients. Separation results are given for a stereophonic recording of three sources.},
	Address = {Dordrecht},
	Author = {F{\'e}votte,C{\'e}dric},
	Date-Added = {2015-03-04 05:37:09 +0000},
	Date-Modified = {2015-03-04 05:37:17 +0000},
	Isbn = {9781402064784; 1402064780},
	Keywords = {Engineering; Microwaves, RF and Optical Engineering; Communications Engineering, Networks; Signal, Image and Speech Processing},
	Language = {English},
	Pages = {305-335},
	Publisher = {Springer Netherlands},
	Title = {Bayesian Audio Source Separation},
	Url = {www.summon.com},
	Year = {2007},
	Bdsk-Url-1 = {www.summon.com}}

@article{fasst,
	Author = {Ozerov, Alexey and Vincent, Emmanuel and Bimbot, Fr{\'e}d{\'e}ric},
	Date-Added = {2015-03-04 05:18:36 +0000},
	Date-Modified = {2015-03-04 05:18:45 +0000},
	Hal_Id = {hal-00626962},
	Hal_Version = {v2},
	Journal = {{IEEE Transactions on Audio, Speech and Language Processing}},
	Keywords = {Audio source separation ; local Gaussian model ; nonnegative matrix factorization ; expectation-maximization},
	Month = May,
	Note = {16},
	Number = {4},
	Pages = {1118 - 1133},
	Publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
	Title = {{A General Flexible Framework for the Handling of Prior Information in Audio Source Separation}},
	Url = {https://hal.archives-ouvertes.fr/hal-00626962},
	Volume = {20},
	Year = {2012},
	Bdsk-Url-1 = {https://hal.archives-ouvertes.fr/hal-00626962}}

@inproceedings{liutkus:hal-00958661,
	Address = {Paris, France},
	Author = {Liutkus, Antoine and Durrieu, Jean-Louis and Daudet, Laurent and Richard, Ga{\"e}l},
	Booktitle = {{WIAMIS}},
	Date-Added = {2015-03-04 04:52:18 +0000},
	Date-Modified = {2015-03-04 04:52:18 +0000},
	Doi = {10.1109/WIAMIS.2013.6616139},
	Hal_Id = {hal-00958661},
	Hal_Version = {v1},
	Keywords = {informed source separation ; source separation ; audio processing},
	Pages = {1-4},
	Title = {{An overview of informed audio source separation}},
	Url = {https://hal.archives-ouvertes.fr/hal-00958661},
	Year = {2013},
	Bdsk-Url-1 = {https://hal.archives-ouvertes.fr/hal-00958661},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/WIAMIS.2013.6616139}}

@misc{peass,
	Author = {Emiya, Valentin and Vincent, Emmanuel and Harlander, Niklas and Hohmann, Volker},
	Date-Added = {2015-03-04 04:19:27 +0000},
	Date-Modified = {2015-03-04 04:19:37 +0000},
	Hal_Id = {inria-00545477},
	Hal_Version = {v1},
	Howpublished = {{9th Int. Conf. on Latent Variable Analysis and Signal Separation}},
	Month = Sep,
	Title = {{The PEASS Toolkit - Perceptual Evaluation methods for Audio Source Separation}},
	Url = {https://hal.inria.fr/inria-00545477},
	Year = {2010},
	Bdsk-Url-1 = {https://hal.inria.fr/inria-00545477}}

@article{doping,
	Abstract = {The separation of an underdetermined audio mixture can be performed through sparse component analysis (SCA) that relies however on the strong hypothesis that source signals are sparse in some domain. To overcome this difficulty in the case where the original sources are available before the mixing process, the informed source separation (ISS) embeds in the mixture a watermark, which information can help a further separation. Though powerful, this technique is generally specific to a particular mixing setup and may be compromised by an additional bitrate compression stage. Thus, instead of watermarking, we propose a `doping' method that makes the time-frequency representation of each source more sparse, while preserving its audio quality. This method is based on an iterative decrease of the distance between the distribution of the signal and a target sparse distribution, under a perceptual constraint. We aim to show that the proposed approach is robust to audio coding and that the use of the sparsified signals improves the source separation, in comparison with the original sources. In this work, the analysis is made only in instantaneous mixtures and focused on voice sources.; The separation of an underdetermined audio mixture can be performed through sparse component analysis (SCA) that relies however on the strong hypothesis that source signals are sparse in some domain. To overcome this difficulty in the case where the original sources are available before the mixing process, the informed source separation (ISS) embeds in the mixture a watermark, which information can help a further separation. Though powerful, this technique is generally specific to a particular mixing setup and may be compromised by an additional bitrate compression stage. Thus, instead of watermarking, we propose a 'doping' method that makes the time-frequency representation of each source more sparse, while preserving its audio quality. This method is based on an iterative decrease of the distance between the distribution of the signal and a target sparse distribution, under a perceptual constraint. We aim to show that the proposed approach is robust to audio coding and that the use of the sparsified signals improves the source separation, in comparison with the original sources. In this work, the analysis is made only in instantaneous mixtures and focused on voice sources.},
	Author = {Mah{\'e},Ga{\"e}l and Nadalin,Everton Z. and Suyama,Ricardo and Romano,Jo{\~a}o M.},
	Date-Added = {2015-03-04 03:59:45 +0000},
	Date-Modified = {2015-03-04 03:59:54 +0000},
	Isbn = {1687-6180},
	Journal = {EURASIP Journal on Advances in Signal Processing},
	Keywords = {Signal, Image and Speech Processing; Engineering; Sparsification; Sparse component analysis (SCA); Informed source separation (ISS); Doping watermarking},
	Language = {English},
	Number = {1},
	Pages = {1-14},
	Title = {Perceptually controlled doping for audio source separation},
	Url = {www.summon.com},
	Volume = {2014},
	Year = {2014; 2013},
	Bdsk-Url-1 = {www.summon.com}}

@article{REPET,
	Abstract = {  Repetition is a core principle in music. Many musical pieces are characterized by an underlying repeating structure over which varying elements are superimposed. This is especially true for pop songs where a singer often overlays varying vocals on a repeating accompaniment. On this basis, we present the REpeating Pattern Extraction Technique (REPET), a novel and simple approach for separating the repeating "background" from the non-repeating "foreground" in a mixture. The basic idea is to identify the periodically repeating segments in the audio, compare them to a repeating segment model derived from them, and extract the repeating patterns via time-frequency masking. Experiments on data sets of 1,000 song clips and 14 full-track real-world songs showed that this method can be successfully applied for music/voice separation, competing with two recent state-of-the-art approaches. Further experiments showed that REPET can also be used as a preprocessor to pitch detection algorithms to improve melody extraction.;   Repetition is a core principle in music. Many musical pieces are characterized by an underlying repeating structure over which varying elements are superimposed. This is especially true for pop songs where a singer often overlays varying vocals on a repeating accompaniment. On this basis, we present the REpeating Pattern Extraction Technique (REPET), a novel and simple approach for separating the repeating "background" from the non-repeating "foreground" in a mixture. The basic idea is to identify the periodically repeating segments in the audio, compare them to a repeating segment model derived from them, and extract the repeating patterns via time-frequency masking. Experiments on data sets of 1,000 song clips and 14 full-track real-world songs showed that this method can be successfully applied for music/voice separation, competing with two recent state-of-the-art approaches. Further experiments showed that REPET can also be used as a preprocessor to pitch detection algorithms to improve melody extraction.; Repetition is a core principle in music. Many musical pieces are characterized by an underlying repeating structure over which varying elements are superimposed. This is especially true for pop songs where a singer often overlays varying vocals on a repeating accompaniment. On this basis, we present the REpeating Pattern Extraction Technique (REPET), a novel and simple approach for separating the repeating "background" from the non-repeating "foreground" in a mixture. The basic idea is to identify the periodically repeating segments in the audio, compare them to a repeating segment model derived from them, and extract the repeating patterns via time-frequency masking. Experiments on data sets of 1,000 song clips and 14 full-track real-world songs showed that this method can be successfully applied for music/voice separation, competing with two recent state-of-the-art approaches. Further experiments showed that REPET can also be used as a preprocessor to pitch detection algorithms to improve melody extraction.},
	Author = {Rafii,Z. and Pardo,B.},
	Date-Added = {2015-03-03 22:54:58 +0000},
	Date-Modified = {2015-03-03 22:55:14 +0000},
	Isbn = {1558-7916},
	Journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	Keywords = {music/voice separation; Adaptation models; Hidden Markov models; Music; Melody extraction; Estimation; music structure analysis; repeating patterns; Speech; Spectrogram; Speech processing; Musical performances},
	Language = {English},
	Number = {1},
	Pages = {73-84},
	Title = {REpeating Pattern Extraction Technique (REPET): A Simple Method for Music/Voice Separation},
	Url = {www.summon.com},
	Volume = {21},
	Year = {2013},
	Bdsk-Url-1 = {www.summon.com}}




@inproceedings{online,
  title={Online REPET-SIM for real-time speech enhancement},
  author={Rafii, Zafar and Pardo, Bryan},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
  pages={848--852},
  year={2013},
  organization={IEEE}
}


@Comment @inproceedings{online,
@Comment 	Abstract = {REPET-SIM is a generalization of the REpeating Pattern Extraction Technique (REPET) that uses a similarity matrix to separate the repeating background from the non-repeating foreground in a mixture. The method assumes that the background (typically the music accompaniment) is dense and low-ranked, while the foreground (typically the singing voice) is sparse and varied. While this assumption is often true for background music and foreground voice in musical mixtures, it also often holds for background noise and foreground speech in noisy mixtures. We therefore propose here to extend REPET-SIM for noise/speech segregation. In particular, given the low computational complexity of the algorithm, we show that the method can be easily implemented online for real-time processing. Evaluation on a data set of 10 stereo two-channel mixtures of speech and real-world background noise showed that this online REPET-SIM can be successfully applied for real-time speech enhancement, performing as well as different competitive methods.},
@Comment 	Author = {Rafii,Zafar and Pardo,Bryan},
@Comment 	Date-Added = {2015-03-03 22:54:02 +0000},
@Comment 	Date-Modified = {2015-03-03 22:54:11 +0000},
@Comment 	Isbn = {1520-6149},
@Comment 	Keywords = {Blind source separation; real-time; similarity matrix; repeating patterns; speech enhancement},
@Comment 	Language = {English},
@Comment 	Pages = {848-852},
@Comment 	Publisher = {IEEE},
@Comment 	Title = {Online REPET-SIM for real-time speech enhancement},
@Comment 	Url = {www.summon.com},
@Comment 	Year = {2013},
@Comment 	Bdsk-Url-1 = {www.summon.com}}



@inproceedings{separation,
  title={A simple music/voice separation method based on the extraction of the repeating musical structure},
  author={Rafii, Zafar and Pardo, Bryan},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on},
  pages={221--224},
  year={2011},
  organization={IEEE}
}

@Comment @inproceedings{separation,
@Comment 	Abstract = {Repetition is a core principle in music. This is especially true for popular songs, generally marked by a noticeable repeating musical structure, over which the singer performs varying lyrics. On this basis, we propose a simple method for separating music and voice, by extraction of the repeating musical structure. First, the period of the repeating structure is found. Then, the spectrogram is segmented at period boundaries and the segments are averaged to create a repeating segment model. Finally, each time-frequency bin in a segment is compared to the model, and the mixture is partitioned using binary time-frequency masking by labeling bins similar to the model as the repeating background. Evaluation on a dataset of 1,000 song clips showed that this method can improve on the performance of an existing music/voice separation method without requiring particular features or complex frameworks.},
@Comment 	Author = {Rafii,Zafar and Pardo,Bryan},
@Comment 	Date-Added = {2015-03-03 22:48:02 +0000},
@Comment 	Date-Modified = {2015-03-03 22:48:14 +0000},
@Comment 	Isbn = {1520-6149},
@Comment 	Keywords = {Binary Time-Frequency Masking; Indexes; Equations; Music/Voice Separation; Time frequency analysis; Music; Mathematical model; Repeating Pattern; Periodic structures; Spectrogram},
@Comment 	Language = {English},
@Comment 	Pages = {221-224},
@Comment 	Publisher = {IEEE},
@Comment 	Title = {A simple music/voice separation method based on the extraction of the repeating musical structure},
@Comment 	Url = {www.summon.com},
@Comment 	Year = {2011},
@Comment 	Bdsk-Url-1 = {www.summon.com}}

@inproceedings{background,
	Abstract = {Background subtraction is a widely used approach for detecting moving objects from static cameras. Many different methods have been proposed over the recent years and both the novice and the expert can be confused about their benefits and limitations. In order to overcome this problem, this paper provides a review of the main methods and an original categorisation based on speed, memory requirements and accuracy. Such a review can effectively guide the designer to select the most suitable method for a given application in a principled way. Methods reviewed include parametric and non-parametric background density estimates and spatial correlation approaches.},
	Author = {Piccardi,M.},
	Date-Added = {2015-03-03 22:46:51 +0000},
	Date-Modified = {2015-03-03 22:47:07 +0000},
	Isbn = {1062-922X},
	Keywords = {background density estimation; feature extraction; object detection; static cameras; background subtraction technique; spatial correlation; image motion analysis},
	Language = {English},
	Pages = {3099-3104 vol.4},
	Title = {Background subtraction techniques: a review},
	Url = {www.summon.com},
	Volume = {4},
	Year = {2004},
	Bdsk-Url-1 = {www.summon.com}}

@article{Rafii:2014aa,
	Abstract = {Musical works are often composed of two characteristic components: the background (typically the musical accompaniment), which generally exhibits a strong rhythmic structure with distinctive repeating time elements, and the melody (typically the singing voice or a solo instrument), which generally exhibits a strong harmonic structure with a distinctive predominant pitch contour. Drawing from findings in cognitive psychology, we propose to investigate the simple combination of two dedicated approaches for separating those two components: a rhythm-based method that focuses on extracting the background via a rhythmic mask derived from identifying the repeating time elements in the mixture and a pitch-based method that focuses on extracting the melody via a harmonic mask derived from identifying the predominant pitch contour in the mixture. Evaluation on a data set of song clips showed that combining such two contrasting yet complementary methods can help to improve separation performance--from the point of view of both components--compared with using only one of those methods, and also compared with two other state-of-the-art approaches.; Musical works are often composed of two characteristic components: the background (typically the musical accompaniment), which generally exhibits a strong rhythmic structure with distinctive repeating time elements, and the melody (typically the singing voice or a solo instrument), which generally exhibits a strong harmonic structure with a distinctive predominant pitch contour. Drawing from findings in cognitive psychology, we propose to investigate the simple combination of two dedicated approaches for separating those two components: a rhythm-based method that focuses on extracting the background via a rhythmic mask derived from identifying the repeating time elements in the mixture and a pitch-based method that focuses on extracting the melody via a harmonic mask derived from identifying the predominant pitch contour in the mixture. Evaluation on a data set of song clips showed that combining such two contrasting yet complementary methods can help to improve separation performance-from the point of view of both components-compared with using only one of those methods, and also compared with two other state-of-the-art approaches.;   Musical works are often composed of two characteristic components: the background (typically the musical accompaniment), which generally exhibits a strong rhythmic structure with distinctive repeating time elements, and the melody (typically the singing voice or a solo instrument), which generally exhibits a strong harmonic structure with a distinctive predominant pitch contour. Drawing from findings in cognitive psychology, we propose to investigate the simple combination of two dedicated approaches for separating those two components: a rhythm-based method that focuses on extracting the background via a rhythmic mask derived from identifying the repeating time elements in the mixture and a pitch-based method that focuses on extracting the melody via a harmonic mask derived from identifying the predominant pitch contour in the mixture. Evaluation on a data set of song clips showed that combining such two contrasting yet complementary methods can help to improve separation performance--from the point of view of both components--compared with using only one of those methods, and also compared with two other state-of-the-art approaches.},
	Address = {Piscataway},
	Author = {Rafii,Zafar and Duan,Zhiyao and Pardo,Bryan},
	Date-Added = {2015-03-03 22:43:31 +0000},
	Date-Modified = {2015-03-03 22:43:31 +0000},
	Doi = {10.1109/TASLP.2014.2354242},
	Id = {1},
	Isbn = {2329-9290},
	Journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	Keywords = {Harmonic analysis; Psychology; separation; Source separation; melody; Background; pitch; Speech; Rhythm; Spectrogram; Speech processing},
	M1 = {Journal Article},
	Number = {12},
	Pages = {1884--1893},
	Publisher = {IEEE},
	Title = {Combining Rhythm-Based and Pitch-Based Methods for Background and Melody Separation},
	Ty = {JOUR},
	U5 = {http://www.syndetics.com/index.aspx?isbn=/sc.gif\&issn=2329-9290\&client=univvicss; http://www.syndetics.com/index.aspx?isbn=/mc.gif\&issn=2329-9290\&client=univvicss; http://www.syndetics.com/index.aspx?isbn=/lc.gif\&issn=2329-9290\&client=univvicss},
	U6 = {ctx{\_}ver=Z39.88-2004\&ctx{\_}enc=info{\%}3Aofi{\%}2Fenc{\%}3AUTF-8\&rfr{\_}id=info:sid/summon.serialssolutions.com\&rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:journal\&rft.genre=article\&rft.atitle=Combining+Rhythm-Based+and+Pitch-Based+Methods+for+Background+and+Melody+Separation\&rft.jtitle=IEEE{\%}2FACM+Transactions+on+Audio{\%}2C+Speech{\%}2C+and+Language+Processing\&rft.au=Rafii{\%}2C+Zafar\&rft.au=Duan{\%}2C+Zhiyao\&rft.au=Pardo{\%}2C+Bryan\&rft.date=2014\&rft.pub=IEEE\&rft.issn=2329-9290\&rft.eissn=2329-9304\&rft.volume=22\&rft.issue=12\&rft.spage=1884\&rft.epage=1893\&rft{\_}id=info:doi/10.1109{\%}2FTASLP.2014.2354242\&rft.externalDocID=6570655\&paramdict=en-US},
	U7 = {Journal Article},
	Url = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwnV1Lb9QwEB6hIqFyKLRAukCl_IGk2fh9bFErJFipYouEuER2bKuIkC3N9rD_vmM7WVp6QOLkWI4ixR7Pw575PgBSl1Xxl06gUjl0FGotMMh1zLfEcMsF91IqSbx8WE0GE8ti4DuJqWiuDI_xZh_7w7E2Q6O7LsIK6puRPINLNa9jZTk2LNVy_TlvqYRSEZQZfQhVoFdQTTU0lTq-PFl-vgiJXrSsCaN1YG6PbCuPVHS0O-cvYMqunfJNtpfQqTbmMajjf_3LS9gbHdP8JEnSPjxx_QE8vwdXeAC7W225eQVL1CQmskvkX64266tfxSkaRJvr3uYXP1AWxv4iUlQPOTrH-aluf4Yykj69tnDdym7ypUv446v-NXw9P7v88LEYGRqKNgDTFdbPpSatMRhnMlxGUVmiheeCKG6MlS7wOLXaOe4rYqgRraeSCSM9w7hPK_IGdvpV7zJ46nG3YYsWMMM5zuDZN_VJLr7Xi9Tdn7rlEKvRyt_rDA1u3KwFK-kh5FQI3jJK0UXR1NbKKNZ61AGW18JbwmYwCxPfXCcYj6Ya57i5tn4GRw_HovPMA0bq3Le0wvFJDBpcr3WoYBuaALQjWI0x3wyy7bjtuoZQdFMDWP_87T--_A52g9ilU533sLO-uXVHCYH2Dsd-8Oc},
	Url1 = {www.summon.com},
	Volume = {22},
	Year = {2014},
	Bdsk-Url-1 = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwnV1Lb9QwEB6hIqFyKLRAukCl_IGk2fh9bFErJFipYouEuER2bKuIkC3N9rD_vmM7WVp6QOLkWI4ixR7Pw575PgBSl1Xxl06gUjl0FGotMMh1zLfEcMsF91IqSbx8WE0GE8ti4DuJqWiuDI_xZh_7w7E2Q6O7LsIK6puRPINLNa9jZTk2LNVy_TlvqYRSEZQZfQhVoFdQTTU0lTq-PFl-vgiJXrSsCaN1YG6PbCuPVHS0O-cvYMqunfJNtpfQqTbmMajjf_3LS9gbHdP8JEnSPjxx_QE8vwdXeAC7W225eQVL1CQmskvkX64266tfxSkaRJvr3uYXP1AWxv4iUlQPOTrH-aluf4Yykj69tnDdym7ypUv446v-NXw9P7v88LEYGRqKNgDTFdbPpSatMRhnMlxGUVmiheeCKG6MlS7wOLXaOe4rYqgRraeSCSM9w7hPK_IGdvpV7zJ46nG3YYsWMMM5zuDZN_VJLr7Xi9Tdn7rlEKvRyt_rDA1u3KwFK-kh5FQI3jJK0UXR1NbKKNZ61AGW18JbwmYwCxPfXCcYj6Ya57i5tn4GRw_HovPMA0bq3Le0wvFJDBpcr3WoYBuaALQjWI0x3wyy7bjtuoZQdFMDWP_87T--_A52g9ilU533sLO-uXVHCYH2Dsd-8Oc},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/TASLP.2014.2354242},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QHS4uLy4uL0Rvd25sb2Fkcy9zY2hvbGFyLTMucmlz0hcLGBlXTlMuZGF0YU8RAYYAAAAAAYYAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMxCfHhIKwAAAAUMxQ1zY2hvbGFyLTMucmlzAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABssQa0suTxQAAAAAAAAAAAAIAAgAACSAAAAAAAAAAAAAAAAAAAAAJRG93bmxvYWRzAAAQAAgAAMxC3ugAAAARAAgAANLMBEUAAAABAAwABQzFAAUKrAACDioAAgA6TWFjaW50b3NoIEhEOlVzZXJzOgBjb2xlcGV0ZXJzb246AERvd25sb2FkczoAc2Nob2xhci0zLnJpcwAOABwADQBzAGMAaABvAGwAYQByAC0AMwAuAHIAaQBzAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAKlVzZXJzL2NvbGVwZXRlcnNvbi9Eb3dubG9hZHMvc2Nob2xhci0zLnJpcwATAAEvAAAVAAIAE///AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAK4AswC7AkUCRwJMAlcCYAJuAnICeQKCAocClAKXAqkCrAKxAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAArM=}}


@inproceedings{filtering,
  title={Adaptive filtering for music/voice separation exploiting the repeating musical structure},
  author={Liutkus, Antoine and Rafii, Zafar and Badeau, Roland and Pardo, Bryan and Richard, Ga{\"e}l},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on},
  pages={53--56},
  year={2012},
  organization={IEEE}
}

@Comment @inproceedings{filtering,
@Comment 	Abstract = {The separation of the lead vocals from the background accompaniment in audio recordings is a challenging task. Recently, an efficient method called REPET (REpeating Pattern Extraction Technique) has been proposed to extract the repeating background from the non-repeating foreground. While effective on individual sections of a song, REPET does not allow for variations in the background (e.g. verse vs. chorus), and is thus limited to short excerpts only. We overcome this limitation and generalize REPET to permit the processing of complete musical tracks. The proposed algorithm tracks the period of the repeating structure and computes local estimates of the background pattern. Separation is performed by soft time-frequency masking, based on the deviation between the current observation and the estimated background pattern. Evaluation on a dataset of 14 complete tracks shows that this method can perform at least as well as a recent competitive music/voice separation method, while being computationally efficient.},
@Comment 	Author = {Liutkus,A. and Rafii,Z. and Badeau,R. and Pardo,B. and Richard,G.},
@Comment 	Date-Added = {2015-03-03 22:41:23 +0000},
@Comment 	Date-Modified = {2015-03-03 22:41:58 +0000},
@Comment 	Isbn = {1520-6149},
@Comment 	Keywords = {Multiple signal classification; Time frequency analysis; Source separation; repeating pattern; Estimation; time-frequency masking; Music/voice separation; Speech; adaptive algorithms; Spectrogram; Speech processing},
@Comment 	Language = {English},
@Comment 	Pages = {53-56},
@Comment 	Publisher = {IEEE},
@Comment 	Title = {Adaptive filtering for music/voice separation exploiting the repeating musical structure},
@Comment 	Url = {www.summon.com},
@Comment 	Year = {2012},
@Comment 	Bdsk-Url-1 = {www.summon.com}}

@article{emiya:inria-00567152,
	Author = {Emiya, Valentin and Vincent, Emmanuel and Harlander, Niklas and Hohmann, Volker},
	Date-Added = {2015-03-03 22:39:56 +0000},
	Date-Modified = {2015-03-03 22:39:56 +0000},
	Doi = {10.1109/TASL.2011.2109381},
	Hal_Id = {inria-00567152},
	Hal_Version = {v1},
	Journal = {{IEEE Transactions on Audio, Speech and Language Processing}},
	Keywords = {\# Distortion measurement ; Noise ; Nonlinear distortion ; Protocols ; Speech ; Audio ; objective measure ; quality assessment ; source separation ; subjective test protocol},
	Month = Sep,
	Number = {7},
	Pages = {2046-2057},
	Publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
	Title = {{Subjective and objective quality assessment of audio source separation}},
	Url = {https://hal.inria.fr/inria-00567152},
	Volume = {19},
	Year = {2011},
	Bdsk-Url-1 = {https://hal.inria.fr/inria-00567152},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/TASL.2011.2109381}}

@book{Aramaki:2013aa,
	Abstract = {This book constitutes the thoroughly refereed post-conference proceedings of the 9th International Symposium on Computer Music Modeling and Retrieval, CMMR 2012, held in London, UK, in June 2012. The 28 revised full papers presented were carefully reviewed and selected for inclusion in this volume. The papers have been organized in the following topical sections: music emotion analysis; 3D audio and sound synthesis; computer models of music perception and cognition; music emotion recognition; music information retrieval; film soundtrack and music recommendation; and computational musicology and music education. The volume also includes selected papers from the Cross-Disciplinary Perspectives on Expressive Performance Workshop held within the framework of CMMR 2012.},
	Address = {Berlin, Heidelberg},
	Author = {Aramaki,Mitsuko and Barthet,Mathieu and Kronland-Martinet,Richard and Ystad,S{\o}lvi and SpringerLink (Online service)},
	Date-Added = {2015-03-03 22:39:19 +0000},
	Date-Modified = {2015-03-03 22:39:19 +0000},
	Doi = {10.1007/978-3-642-41248-6},
	Id = {1},
	Isbn = {9783642412486; 3642412483; 9783642412479; 3642412475},
	Keywords = {Music; Information storage and retrieval systems; Multimedia systems; Computer science; Software engineering; Information systems; Computer Appl. in Arts and Humanities; Multimedia Information Systems; User Interfaces and Human Computer Interaction; Special Purpose and Application-Based Systems; Information Storage and Retrieval},
	M1 = {Book, Whole},
	Publisher = {Springer Berlin Heidelberg},
	Title = {From Sounds to Music and Emotions: 9th International Symposium, CMMR 2012, London, UK, June 19-22, 2012, Revised Selected Papers},
	Ty = {BOOK},
	U5 = {http://www.syndetics.com/index.aspx?isbn=9783642412486/sc.gif\&client=univvicss\&freeimage=true; http://www.syndetics.com/index.aspx?isbn=9783642412486/mc.gif\&client=univvicss\&freeimage=true; http://www.syndetics.com/index.aspx?isbn=9783642412486/lc.gif\&client=univvicss\&freeimage=true},
	U6 = {ctx{\_}ver=Z39.88-2004\&ctx{\_}enc=info{\%}3Aofi{\%}2Fenc{\%}3AUTF-8\&rfr{\_}id=info:sid/summon.serialssolutions.com\&rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:book\&rft.genre=book\&rft.title=From+Sounds+to+Music+and+Emotions\&rft.series=Lecture+Notes+in+Computer+Science\&rft.date=2013-01-01\&rft.pub=Springer+Berlin+Heidelberg\&rft.isbn=9783642412479\&rft.volume=7900\&rft{\_}id=info:doi/10.1007{\%}2F978-3-642-41248-6\&rft.externalDBID=n{\%}2Fa\&rft.externalDocID=978-3-642-41248-6{\_}319515\&paramdict=en-US},
	U7 = {eBook},
	Url = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwdV3BTsMwDLXYKiFOsA3KGEjhyCFVSUqbndEqJNgBwQFxidK1ubHBWv4fO2mhmsapci51LSeO7T4_ACmimO-cCaktDM1VoUacSqSwKYZ6k5WpVaLIXEO_hyaDbirE7uTEFnjyf38GvVdgEjGAASZfBOh4fvqruMTEKqQI4IGXbmJdVtKP4PmV014_tDcGtI02-QkEFUEQRnBQrcdw3BEvsHYfjiFw7MwTuM63mw_2QsxINWs2zC0zsy7ZwrPz1Kcwyxev9w-cXqLbWo1u9ZdnMMTsvwohsOiF-MTIEKIWIRy-zR_V8l0svTjqxKh2KK3oqwkxEDkn5ndRcg6svJWqWNkVBvIsMQaPlaSkfpospIzV3Ezhpvtm_elHXGj_0wSahTu78FTjVsWrzxQm-_S92L88gyPh2COoYnEJw2b7XV15s_4A-kaQSg},
	Url1 = {www.summon.com},
	Volume = {7900},
	Year = {2013},
	Bdsk-Url-1 = {http://uvic.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwdV3BTsMwDLXYKiFOsA3KGEjhyCFVSUqbndEqJNgBwQFxidK1ubHBWv4fO2mhmsapci51LSeO7T4_ACmimO-cCaktDM1VoUacSqSwKYZ6k5WpVaLIXEO_hyaDbirE7uTEFnjyf38GvVdgEjGAASZfBOh4fvqruMTEKqQI4IGXbmJdVtKP4PmV014_tDcGtI02-QkEFUEQRnBQrcdw3BEvsHYfjiFw7MwTuM63mw_2QsxINWs2zC0zsy7ZwrPz1Kcwyxev9w-cXqLbWo1u9ZdnMMTsvwohsOiF-MTIEKIWIRy-zR_V8l0svTjqxKh2KK3oqwkxEDkn5ndRcg6svJWqWNkVBvIsMQaPlaSkfpospIzV3Ezhpvtm_elHXGj_0wSahTu78FTjVsWrzxQm-_S92L88gyPh2COoYnEJw2b7XV15s_4A-kaQSg},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-3-642-41248-6}}

@inproceedings{sisec,
	Address = {Tel Aviv, Israel},
	Author = {Araki, Shoko and Nesta, Francesco and Vincent, Emmanuel and Koldovsky, Zbynek and Nolte, Guido and Ziehe, Andreas and Benichoux, Alexis},
	Booktitle = {{10th Int. Conf. on Latent Variable Analysis and Signal Separation (LVA/ICA)}},
	Date-Added = {2015-03-03 22:37:58 +0000},
	Date-Modified = {2015-03-04 04:21:46 +0000},
	Hal_Id = {hal-00655394},
	Hal_Version = {v1},
	Month = Mar,
	Pages = {414-422},
	Title = {{The 2011 Signal Separation Evaluation Campaign (SiSEC2011): - Audio source separation -}},
	Url = {https://hal.inria.fr/hal-00655394},
	Year = {2012},
	Bdsk-Url-1 = {https://hal.inria.fr/hal-00655394}}

@misc{googlepatents,
	Author = {Pardo, B. and Rafii, Z.},
	Date = {2013},
	Date-Added = {2015-03-03 22:32:20 +0000},
	Date-Modified = {2015-03-03 22:33:55 +0000},
	Publisher = {Google Patents},
	Title = {Audio separation system and method},
	Url = {https://www.google.com/patents/US20130064379},
	Year = {2013},
	Bdsk-Url-1 = {https://www.google.com/patents/US20130064379}}
